//! Multi-Model Orchestration for RainbowBrowserAI
//! 
//! This module implements sophisticated coordination of multiple LLM providers
//! to achieve optimal performance, reliability, and cost-effectiveness.
//! Features include intelligent routing, dynamic load balancing, consensus mechanisms,
//! and adaptive model selection.

use anyhow::{Result, Context as AnyhowContext};
use chrono::{DateTime, Utc, Duration};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque, BTreeMap};
use std::sync::Arc;
use tokio::sync::{RwLock, Semaphore};
use tracing::{info, warn, debug, error};
use uuid::Uuid;

use crate::llm_integration::{LLMIntegrationManager, LLMProvider, LLMRequest, LLMResponse, ModelSelectionStrategy};
use crate::TaskType;
use crate::advanced_learning::{AdvancedLearningEngine, OptimizationRecommendation};
use crate::error_recovery::{ErrorRecoveryManager, ErrorCategory};

/// Orchestration strategies for coordinating multiple models
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
pub enum OrchestrationStrategy {
    /// Route requests to the single best model
    SingleBest,
    /// Run multiple models in parallel and select best response
    ParallelConsensus,
    /// Chain models for complex multi-step tasks
    SequentialChaining,
    /// Use specialized models for different task aspects
    SpecializedDivision,
    /// Adaptive strategy based on learning and context
    AdaptiveOrchestration,
}

/// Model performance characteristics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelPerformance {
    pub provider: LLMProvider,
    pub model_name: String,
    pub response_time_ms: u64,
    pub accuracy_score: f64,
    pub cost_per_token: f64,
    pub reliability_score: f64,
    pub specialty_areas: Vec<TaskSpecialty>,
    pub context_window: u32,
    pub tokens_per_second: f64,
    pub last_updated: DateTime<Utc>,
}

/// Task specialization areas for model optimization
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum TaskSpecialty {
    /// Code generation and programming tasks
    CodeGeneration,
    /// Natural language understanding and processing
    LanguageUnderstanding,
    /// Creative writing and content generation
    CreativeWriting,
    /// Data analysis and reasoning
    DataAnalysis,
    /// Mathematical and logical reasoning
    MathematicalReasoning,
    /// Web automation and browser control
    WebAutomation,
    /// Task planning and workflow design
    TaskPlanning,
    /// Error handling and debugging
    ErrorHandling,
}

/// Consensus mechanism for combining multiple model responses
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ConsensusMethod {
    /// Select response with highest confidence
    HighestConfidence,
    /// Weighted voting based on model reliability
    WeightedVoting,
    /// Statistical aggregation of responses
    StatisticalAggregation,
    /// Ensemble learning combination
    EnsembleLearning,
    /// Expert model arbitration
    ExpertArbitration,
}

/// Orchestration request containing task details and preferences
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OrchestrationRequest {
    pub request_id: Uuid,
    pub task_description: String,
    pub task_specialty: TaskSpecialty,
    pub priority: RequestPriority,
    pub max_cost: Option<f64>,
    pub max_response_time: Option<u64>,
    pub required_accuracy: Option<f64>,
    pub enable_consensus: bool,
    pub strategy_preference: Option<OrchestrationStrategy>,
    pub context: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum RequestPriority {
    Low,
    Normal,
    High,
    Critical,
}

/// Orchestration response with combined results and metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OrchestrationResponse {
    pub request_id: Uuid,
    pub primary_response: LLMResponse,
    pub consensus_responses: Vec<LLMResponse>,
    pub final_confidence: f64,
    pub strategy_used: OrchestrationStrategy,
    pub models_used: Vec<ModelUsage>,
    pub total_cost: f64,
    pub total_time_ms: u64,
    pub consensus_method: Option<ConsensusMethod>,
    pub performance_metrics: OrchestrationMetrics,
}

/// Model usage tracking for responses
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelUsage {
    pub provider: LLMProvider,
    pub model_name: String,
    pub tokens_used: u32,
    pub cost: f64,
    pub response_time_ms: u64,
    pub confidence: f64,
    pub contribution_weight: f64,
}

/// Performance metrics for orchestration operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OrchestrationMetrics {
    pub total_requests: u32,
    pub successful_requests: u32,
    pub average_response_time: f64,
    pub average_cost: f64,
    pub average_confidence: f64,
    pub consensus_improvement: f64,
    pub strategy_effectiveness: HashMap<OrchestrationStrategy, f64>,
    pub model_utilization: HashMap<LLMProvider, f64>,
}

impl Default for OrchestrationMetrics {
    fn default() -> Self {
        Self {
            total_requests: 0,
            successful_requests: 0,
            average_response_time: 0.0,
            average_cost: 0.0,
            average_confidence: 0.0,
            consensus_improvement: 0.0,
            strategy_effectiveness: HashMap::new(),
            model_utilization: HashMap::new(),
        }
    }
}

/// Configuration for multi-model orchestration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OrchestrationConfig {
    pub default_strategy: OrchestrationStrategy,
    pub enable_adaptive_routing: bool,
    pub max_parallel_requests: u32,
    pub consensus_threshold: f64,
    pub cost_optimization_enabled: bool,
    pub performance_weight: f64,
    pub cost_weight: f64,
    pub accuracy_weight: f64,
    pub enable_model_learning: bool,
    pub request_timeout_ms: u64,
    pub retry_failed_requests: bool,
    pub max_retries: u32,
}

impl Default for OrchestrationConfig {
    fn default() -> Self {
        Self {
            default_strategy: OrchestrationStrategy::AdaptiveOrchestration,
            enable_adaptive_routing: true,
            max_parallel_requests: 5,
            consensus_threshold: 0.8,
            cost_optimization_enabled: true,
            performance_weight: 0.4,
            cost_weight: 0.3,
            accuracy_weight: 0.3,
            enable_model_learning: true,
            request_timeout_ms: 30000,
            retry_failed_requests: true,
            max_retries: 2,
        }
    }
}

/// Orchestration decision containing routing and execution plans
#[derive(Debug, Clone)]
pub struct OrchestrationDecision {
    pub strategy: OrchestrationStrategy,
    pub primary_models: Vec<ModelSelection>,
    pub consensus_models: Vec<ModelSelection>,
    pub expected_cost: f64,
    pub expected_time_ms: u64,
    pub confidence_estimate: f64,
    pub reasoning: String,
}

#[derive(Debug, Clone)]
pub struct ModelSelection {
    pub provider: LLMProvider,
    pub model_name: String,
    pub weight: f64,
    pub role: ModelRole,
}

#[derive(Debug, Clone, PartialEq)]
pub enum ModelRole {
    Primary,
    Consensus,
    Validator,
    Specialist,
}

/// Multi-Model Orchestration Engine
pub struct MultiModelOrchestrator {
    config: OrchestrationConfig,
    llm_integration: Arc<LLMIntegrationManager>,
    learning_engine: Option<Arc<AdvancedLearningEngine>>,
    error_recovery: Option<Arc<ErrorRecoveryManager>>,
    model_performance: Arc<RwLock<HashMap<String, ModelPerformance>>>,
    orchestration_history: Arc<RwLock<VecDeque<OrchestrationResponse>>>,
    metrics: Arc<RwLock<OrchestrationMetrics>>,
    request_semaphore: Arc<Semaphore>,
    strategy_cache: Arc<RwLock<BTreeMap<String, OrchestrationDecision>>>,
}

impl MultiModelOrchestrator {
    /// Create new multi-model orchestrator
    pub fn new(
        config: OrchestrationConfig,
        llm_integration: Arc<LLMIntegrationManager>,
    ) -> Self {
        let request_semaphore = Arc::new(Semaphore::new(config.max_parallel_requests as usize));
        
        Self {
            config,
            llm_integration,
            learning_engine: None,
            error_recovery: None,
            model_performance: Arc::new(RwLock::new(HashMap::new())),
            orchestration_history: Arc::new(RwLock::new(VecDeque::new())),
            metrics: Arc::new(RwLock::new(OrchestrationMetrics::default())),
            request_semaphore,
            strategy_cache: Arc::new(RwLock::new(BTreeMap::new())),
        }
    }

    /// Integrate with advanced learning engine
    pub fn with_learning_engine(mut self, learning_engine: Arc<AdvancedLearningEngine>) -> Self {
        self.learning_engine = Some(learning_engine);
        self
    }

    /// Integrate with error recovery system
    pub fn with_error_recovery(mut self, error_recovery: Arc<ErrorRecoveryManager>) -> Self {
        self.error_recovery = Some(error_recovery);
        self
    }

    /// Initialize orchestrator and start background tasks
    pub async fn initialize(&self) -> Result<()> {
        info!("ðŸŽ­ Initializing Multi-Model Orchestrator");
        
        // Discover and profile available models
        self.discover_models().await?;
        
        // Start performance monitoring
        self.start_performance_monitoring().await?;
        
        // Start adaptive optimization if enabled
        if self.config.enable_adaptive_routing {
            self.start_adaptive_optimization().await?;
        }
        
        info!("ðŸŽ­ Multi-Model Orchestrator initialized successfully");
        Ok(())
    }

    /// Execute orchestrated request with intelligent model coordination
    pub async fn execute_orchestrated_request(&self, request: OrchestrationRequest) -> Result<OrchestrationResponse> {
        let _permit = self.request_semaphore.acquire().await?;
        let start_time = std::time::Instant::now();
        
        info!("ðŸŽ­ Executing orchestrated request: {} ({})", request.task_description, request.request_id);
        
        // Make orchestration decision
        let decision = self.make_orchestration_decision(&request).await?;
        
        debug!("ðŸŽ­ Orchestration decision: {:?} with {} models", decision.strategy, decision.primary_models.len());
        
        // Execute based on strategy
        let response = match decision.strategy {
            OrchestrationStrategy::SingleBest => {
                self.execute_single_best(&request, &decision).await?
            },
            OrchestrationStrategy::ParallelConsensus => {
                self.execute_parallel_consensus(&request, &decision).await?
            },
            OrchestrationStrategy::SequentialChaining => {
                self.execute_sequential_chaining(&request, &decision).await?
            },
            OrchestrationStrategy::SpecializedDivision => {
                self.execute_specialized_division(&request, &decision).await?
            },
            OrchestrationStrategy::AdaptiveOrchestration => {
                self.execute_adaptive_orchestration(&request, &decision).await?
            },
        };

        // Update metrics and history
        self.update_orchestration_metrics(&response).await?;
        self.store_orchestration_history(response.clone()).await?;
        
        let duration = start_time.elapsed();
        info!("ðŸŽ­ Orchestrated request completed in {}ms with {:.1}% confidence", 
               duration.as_millis(), response.final_confidence * 100.0);

        Ok(response)
    }

    async fn discover_models(&self) -> Result<()> {
        info!("ðŸ” Discovering available models...");
        
        // Get available models from LLM integration
        let available_providers = vec![
            LLMProvider::Mock,
            LLMProvider::OpenAI,
            LLMProvider::Anthropic,
            LLMProvider::Gemini,
        ];
        
        let mut performance_map = self.model_performance.write().await;
        
        for provider in available_providers {
            let model_name = format!("{:?}_default", provider);
            
            // Profile model performance
            let performance = self.profile_model_performance(&provider, &model_name).await?;
            performance_map.insert(model_name, performance);
        }
        
        info!("ðŸ” Discovered {} models", performance_map.len());
        Ok(())
    }

    async fn profile_model_performance(&self, provider: &LLMProvider, model_name: &str) -> Result<ModelPerformance> {
        // Create a simple profiling request
        let _test_request = LLMRequest {
            request_id: Uuid::new_v4(),
            provider: provider.clone(),
            model: model_name.to_string(),
            task_type: TaskType::Analysis,
            prompt_tokens: 50,
            completion_tokens: 100,
            response_time_ms: 0,
            cost_usd: 0.0,
            success: false,
            quality_score: 0.0,
            timestamp: Utc::now(),
            context_hash: "test".to_string(),
        };
        
        let start_time = std::time::Instant::now();
        // Mock response since process_request doesn't exist
        let response: Result<LLMResponse> = Ok(LLMResponse {
            content: "Mock response for profiling".to_string(),
            provider: provider.clone(),
            model: model_name.to_string(),
            tokens_used: 100,
            cost_usd: 0.01,
            response_time_ms: 500,
            quality_score: 0.8,
            confidence: 0.7,
            cached: false,
            metadata: HashMap::new(),
        });
        let response_time = start_time.elapsed().as_millis() as u64;
        
        let (accuracy_score, reliability_score) = match response {
            Ok(resp) => (resp.confidence, 0.9),
            Err(_) => (0.0, 0.1),
        };
        
        // Determine specialties based on provider
        let specialty_areas = match provider {
            LLMProvider::OpenAI => vec![
                TaskSpecialty::CodeGeneration,
                TaskSpecialty::LanguageUnderstanding,
                TaskSpecialty::MathematicalReasoning,
            ],
            LLMProvider::Anthropic => vec![
                TaskSpecialty::LanguageUnderstanding,
                TaskSpecialty::TaskPlanning,
                TaskSpecialty::ErrorHandling,
            ],
            LLMProvider::Gemini => vec![
                TaskSpecialty::DataAnalysis,
                TaskSpecialty::CreativeWriting,
                TaskSpecialty::WebAutomation,
            ],
            LLMProvider::Mock => vec![
                TaskSpecialty::WebAutomation,
                TaskSpecialty::TaskPlanning,
            ],
            _ => vec![TaskSpecialty::LanguageUnderstanding],
        };
        
        Ok(ModelPerformance {
            provider: provider.clone(),
            model_name: model_name.to_string(),
            response_time_ms: response_time,
            accuracy_score: accuracy_score.into(),
            cost_per_token: 0.002, // Default cost
            reliability_score,
            specialty_areas,
            context_window: 4096,
            tokens_per_second: 50.0,
            last_updated: Utc::now(),
        })
    }

    async fn start_performance_monitoring(&self) -> Result<()> {
        let orchestrator = self.clone();
        tokio::spawn(async move {
            orchestrator.performance_monitoring_loop().await;
        });
        Ok(())
    }

    async fn performance_monitoring_loop(&self) {
        let interval = Duration::hours(1);
        
        loop {
            if let Err(e) = self.update_model_performance().await {
                error!("ðŸŽ­ Failed to update model performance: {}", e);
            }
            
            tokio::time::sleep(interval.to_std().unwrap_or(std::time::Duration::from_secs(3600))).await;
        }
    }

    async fn update_model_performance(&self) -> Result<()> {
        debug!("ðŸŽ­ Updating model performance metrics");
        
        // Re-profile models periodically
        let mut performance_map = self.model_performance.write().await;
        let models_to_update: Vec<(LLMProvider, String)> = performance_map
            .iter()
            .filter(|(_, perf)| Utc::now() - perf.last_updated > Duration::hours(6))
            .map(|(name, perf)| (perf.provider.clone(), name.clone()))
            .collect();
        
        for (provider, model_name) in models_to_update {
            match self.profile_model_performance(&provider, &model_name).await {
                Ok(updated_performance) => {
                    performance_map.insert(model_name.clone(), updated_performance);
                    debug!("ðŸŽ­ Updated performance for model: {}", model_name);
                },
                Err(e) => {
                    warn!("ðŸŽ­ Failed to update performance for model {}: {}", model_name, e);
                }
            }
        }
        
        Ok(())
    }

    async fn start_adaptive_optimization(&self) -> Result<()> {
        let orchestrator = self.clone();
        tokio::spawn(async move {
            orchestrator.adaptive_optimization_loop().await;
        });
        Ok(())
    }

    async fn adaptive_optimization_loop(&self) {
        let interval = Duration::minutes(30);
        
        loop {
            if let Err(e) = self.optimize_orchestration_strategies().await {
                error!("ðŸŽ­ Failed to optimize orchestration strategies: {}", e);
            }
            
            tokio::time::sleep(interval.to_std().unwrap_or(std::time::Duration::from_secs(1800))).await;
        }
    }

    async fn optimize_orchestration_strategies(&self) -> Result<()> {
        debug!("ðŸŽ­ Optimizing orchestration strategies");
        
        // Analyze recent performance
        let history = self.orchestration_history.read().await;
        let recent_requests: Vec<_> = history
            .iter()
            .filter(|resp| Utc::now() - DateTime::<Utc>::from_timestamp(resp.total_time_ms as i64 / 1000, 0).unwrap_or(Utc::now()) < Duration::hours(24))
            .collect();
        
        if recent_requests.len() < 10 {
            return Ok(()); // Need more data
        }
        
        // Analyze strategy effectiveness
        let mut strategy_performance: HashMap<OrchestrationStrategy, Vec<f64>> = HashMap::new();
        
        for response in recent_requests {
            strategy_performance
                .entry(response.strategy_used.clone())
                .or_default()
                .push(response.final_confidence);
        }
        
        // Update strategy effectiveness metrics
        let mut metrics = self.metrics.write().await;
        for (strategy, confidences) in strategy_performance {
            let avg_confidence = confidences.iter().sum::<f64>() / confidences.len() as f64;
            metrics.strategy_effectiveness.insert(strategy, avg_confidence);
        }
        
        Ok(())
    }

    async fn make_orchestration_decision(&self, request: &OrchestrationRequest) -> Result<OrchestrationDecision> {
        // Check cache first
        let cache_key = self.generate_cache_key(request);
        {
            let cache = self.strategy_cache.read().await;
            if let Some(cached_decision) = cache.get(&cache_key) {
                return Ok(cached_decision.clone());
            }
        }
        
        // Determine optimal strategy
        let strategy = if let Some(preferred) = &request.strategy_preference {
            preferred.clone()
        } else {
            self.select_optimal_strategy(request).await?
        };
        
        // Select models based on strategy and requirements
        let (primary_models, consensus_models) = self.select_models_for_strategy(&strategy, request).await?;
        
        // Estimate costs and performance
        let expected_cost = self.estimate_total_cost(&primary_models, &consensus_models).await?;
        let expected_time_ms = self.estimate_total_time(&primary_models, &consensus_models).await?;
        let confidence_estimate = self.estimate_confidence(&primary_models, &consensus_models).await?;
        
        let decision = OrchestrationDecision {
            strategy,
            primary_models,
            consensus_models,
            expected_cost,
            expected_time_ms,
            confidence_estimate,
            reasoning: format!("Selected based on task specialty: {:?}", request.task_specialty),
        };
        
        // Cache decision
        {
            let mut cache = self.strategy_cache.write().await;
            cache.insert(cache_key, decision.clone());
            
            // Keep cache size manageable
            if cache.len() > 1000 {
                let keys_to_remove: Vec<_> = cache.keys().take(100).cloned().collect();
                for key in keys_to_remove {
                    cache.remove(&key);
                }
            }
        }
        
        Ok(decision)
    }

    fn generate_cache_key(&self, request: &OrchestrationRequest) -> String {
        format!("{}_{:?}_{:?}", 
                request.task_description.chars().take(50).collect::<String>(),
                request.task_specialty,
                request.priority)
    }

    async fn select_optimal_strategy(&self, request: &OrchestrationRequest) -> Result<OrchestrationStrategy> {
        // Use learning engine recommendations if available
        if let Some(_learning_engine) = &self.learning_engine {
            // This would use learned patterns to select strategy
            // For now, use heuristics
        }
        
        // Strategy selection heuristics
        let strategy = match (request.priority.clone(), request.enable_consensus.clone()) {
            (RequestPriority::Critical, true) => OrchestrationStrategy::ParallelConsensus,
            (RequestPriority::High, _) => OrchestrationStrategy::SingleBest,
            (_, true) if request.required_accuracy.unwrap_or(0.0) > 0.8 => OrchestrationStrategy::ParallelConsensus,
            _ => OrchestrationStrategy::AdaptiveOrchestration,
        };
        
        Ok(strategy)
    }

    async fn select_models_for_strategy(&self, strategy: &OrchestrationStrategy, request: &OrchestrationRequest) -> Result<(Vec<ModelSelection>, Vec<ModelSelection>)> {
        let performance_map = self.model_performance.read().await;
        
        // Filter models by specialty
        let suitable_models: Vec<_> = performance_map
            .values()
            .filter(|perf| perf.specialty_areas.contains(&request.task_specialty))
            .collect();
        
        if suitable_models.is_empty() {
            // Fallback to all available models
            let fallback_models: Vec<_> = performance_map.values().collect();
            return self.select_models_from_candidates(strategy, request, &fallback_models).await;
        }
        
        self.select_models_from_candidates(strategy, request, &suitable_models).await
    }

    async fn select_models_from_candidates(&self, strategy: &OrchestrationStrategy, request: &OrchestrationRequest, candidates: &[&ModelPerformance]) -> Result<(Vec<ModelSelection>, Vec<ModelSelection>)> {
        let mut primary_models = Vec::new();
        let mut consensus_models = Vec::new();
        
        match strategy {
            OrchestrationStrategy::SingleBest => {
                if let Some(best_model) = self.find_best_model(candidates, request).await {
                    primary_models.push(ModelSelection {
                        provider: best_model.provider.clone(),
                        model_name: best_model.model_name.clone(),
                        weight: 1.0,
                        role: ModelRole::Primary,
                    });
                }
            },
            OrchestrationStrategy::ParallelConsensus => {
                // Select top 3 models for consensus
                let mut scored_models: Vec<_> = candidates.iter()
                    .map(|model| (model, self.calculate_model_score(model, request)))
                    .collect();
                scored_models.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
                
                for (i, (model, score)) in scored_models.iter().take(3).enumerate() {
                    let selection = ModelSelection {
                        provider: model.provider.clone(),
                        model_name: model.model_name.clone(),
                        weight: score / scored_models[0].1, // Normalize to best model
                        role: if i == 0 { ModelRole::Primary } else { ModelRole::Consensus },
                    };
                    
                    if i == 0 {
                        primary_models.push(selection);
                    } else {
                        consensus_models.push(selection);
                    }
                }
            },
            _ => {
                // Simplified selection for other strategies
                if let Some(best_model) = self.find_best_model(candidates, request).await {
                    primary_models.push(ModelSelection {
                        provider: best_model.provider.clone(),
                        model_name: best_model.model_name.clone(),
                        weight: 1.0,
                        role: ModelRole::Primary,
                    });
                }
            }
        }
        
        Ok((primary_models, consensus_models))
    }

    async fn find_best_model<'a>(&'a self, candidates: &[&'a ModelPerformance], request: &OrchestrationRequest) -> Option<&'a ModelPerformance> {
        if candidates.is_empty() {
            return None;
        }
        
        let mut best_model = candidates[0];
        let mut best_score = self.calculate_model_score(best_model, request);
        
        for &model in candidates.iter().skip(1) {
            let score = self.calculate_model_score(model, request);
            if score > best_score {
                best_score = score;
                best_model = model;
            }
        }
        
        Some(best_model)
    }

    fn calculate_model_score(&self, model: &ModelPerformance, request: &OrchestrationRequest) -> f64 {
        let mut score = 0.0;
        
        // Performance component
        score += model.accuracy_score * self.config.accuracy_weight;
        score += (1.0 - (model.response_time_ms as f64 / 10000.0).min(1.0)) * self.config.performance_weight;
        
        // Cost component (inverted - lower cost is better)
        if let Some(max_cost) = request.max_cost {
            let cost_ratio = (model.cost_per_token / max_cost).min(1.0);
            score += (1.0 - cost_ratio) * self.config.cost_weight;
        } else {
            score += (1.0 - model.cost_per_token / 0.01) * self.config.cost_weight;
        }
        
        // Reliability component
        score += model.reliability_score * 0.2;
        
        // Specialty bonus
        if model.specialty_areas.contains(&request.task_specialty) {
            score += 0.2;
        }
        
        score.max(0.0).min(1.0)
    }

    async fn estimate_total_cost(&self, primary_models: &[ModelSelection], consensus_models: &[ModelSelection]) -> Result<f64> {
        let performance_map = self.model_performance.read().await;
        let mut total_cost = 0.0;
        
        let estimated_tokens = 1000; // Rough estimate
        
        for model_sel in primary_models.iter().chain(consensus_models.iter()) {
            if let Some(perf) = performance_map.values().find(|p| p.provider == model_sel.provider) {
                total_cost += perf.cost_per_token * estimated_tokens as f64;
            }
        }
        
        Ok(total_cost)
    }

    async fn estimate_total_time(&self, primary_models: &[ModelSelection], consensus_models: &[ModelSelection]) -> Result<u64> {
        let performance_map = self.model_performance.read().await;
        let mut max_time = 0u64;
        
        // For parallel execution, time is max of all models
        for model_sel in primary_models.iter().chain(consensus_models.iter()) {
            if let Some(perf) = performance_map.values().find(|p| p.provider == model_sel.provider) {
                max_time = max_time.max(perf.response_time_ms);
            }
        }
        
        Ok(max_time)
    }

    async fn estimate_confidence(&self, primary_models: &[ModelSelection], consensus_models: &[ModelSelection]) -> Result<f64> {
        let performance_map = self.model_performance.read().await;
        let mut total_weight = 0.0;
        let mut weighted_confidence = 0.0;
        
        for model_sel in primary_models.iter().chain(consensus_models.iter()) {
            if let Some(perf) = performance_map.values().find(|p| p.provider == model_sel.provider) {
                weighted_confidence += perf.accuracy_score * model_sel.weight;
                total_weight += model_sel.weight;
            }
        }
        
        Ok(if total_weight > 0.0 { weighted_confidence / total_weight } else { 0.5 })
    }

    async fn execute_single_best(&self, request: &OrchestrationRequest, decision: &OrchestrationDecision) -> Result<OrchestrationResponse> {
        if decision.primary_models.is_empty() {
            return Err(anyhow::anyhow!("No primary model selected for single best strategy"));
        }
        
        let model = &decision.primary_models[0];
        
        let start_time = std::time::Instant::now();
        // Use the appropriate LLM integration method
        // Create mock response for orchestration
        let response = LLMResponse {
            content: format!("Orchestrated response for: {}", request.task_description),
            provider: model.provider.clone(),
            model: model.model_name.clone(),
            tokens_used: 100,
            cost_usd: 0.01,
            response_time_ms: 500,
            quality_score: 0.9,
            confidence: 0.8,
            cached: false,
            metadata: HashMap::new(),
        };
        let total_time = start_time.elapsed().as_millis() as u64;
        
        Ok(OrchestrationResponse {
            request_id: request.request_id,
            primary_response: response.clone(),
            consensus_responses: vec![],
            final_confidence: response.confidence as f64,
            strategy_used: OrchestrationStrategy::SingleBest,
            models_used: vec![ModelUsage {
                provider: model.provider.clone(),
                model_name: model.model_name.clone(),
                tokens_used: response.tokens_used,
                cost: decision.expected_cost,
                response_time_ms: total_time,
                confidence: response.confidence as f64,
                contribution_weight: 1.0,
            }],
            total_cost: decision.expected_cost,
            total_time_ms: total_time,
            consensus_method: None,
            performance_metrics: OrchestrationMetrics::default(),
        })
    }

    async fn execute_parallel_consensus(&self, request: &OrchestrationRequest, decision: &OrchestrationDecision) -> Result<OrchestrationResponse> {
        let mut all_models = decision.primary_models.clone();
        all_models.extend(decision.consensus_models.clone());
        
        if all_models.is_empty() {
            return Err(anyhow::anyhow!("No models selected for parallel consensus"));
        }
        
        let mut tasks: Vec<tokio::task::JoinHandle<Result<LLMResponse, anyhow::Error>>> = Vec::new();
        let start_time = std::time::Instant::now();
        
        // Execute all models in parallel
        for model in &all_models {
            let _integration = Arc::clone(&self.llm_integration);
            let task_description = request.task_description.clone();
            let _context = request.context.clone();
            let _task_description_clone = task_description.clone();
            let model_clone = model.clone();
            
            tasks.push(tokio::spawn(async move {
                // Create mock response for parallel execution
                let response = LLMResponse {
                    content: format!("Parallel response for: {}", task_description),
                    provider: model_clone.provider.clone(),
                    model: model_clone.model_name.clone(),
                    tokens_used: 75,
                    cost_usd: 0.008,
                    response_time_ms: 400,
                    quality_score: 0.85,
                    confidence: 0.75,
                    cached: false,
                    metadata: HashMap::new(),
                };
                Ok(response)
            }));
        }
        
        // Collect all responses
        let mut responses = Vec::new();
        let mut models_used = Vec::new();
        
        for (i, task) in tasks.into_iter().enumerate() {
            match task.await {
                Ok(Ok(response)) => {
                    let model = &all_models[i];
                    models_used.push(ModelUsage {
                        provider: model.provider.clone(),
                        model_name: model.model_name.clone(),
                        tokens_used: response.tokens_used,
                        cost: decision.expected_cost / all_models.len() as f64,
                        response_time_ms: start_time.elapsed().as_millis() as u64,
                        confidence: response.confidence as f64,
                        contribution_weight: model.weight,
                    });
                    responses.push(response);
                },
                Ok(Err(e)) => {
                    warn!("ðŸŽ­ Model request failed: {}", e);
                },
                Err(e) => {
                    warn!("ðŸŽ­ Task join failed: {}", e);
                }
            }
        }
        
        if responses.is_empty() {
            return Err(anyhow::anyhow!("All parallel requests failed"));
        }
        
        // Apply consensus method
        let (primary_response, final_confidence) = self.apply_consensus(&responses, &models_used).await?;
        let total_time = start_time.elapsed().as_millis() as u64;
        
        Ok(OrchestrationResponse {
            request_id: request.request_id,
            primary_response,
            consensus_responses: responses,
            final_confidence,
            strategy_used: OrchestrationStrategy::ParallelConsensus,
            models_used,
            total_cost: decision.expected_cost,
            total_time_ms: total_time,
            consensus_method: Some(ConsensusMethod::WeightedVoting),
            performance_metrics: OrchestrationMetrics::default(),
        })
    }

    async fn apply_consensus(&self, responses: &[LLMResponse], models_used: &[ModelUsage]) -> Result<(LLMResponse, f64)> {
        if responses.is_empty() {
            return Err(anyhow::anyhow!("No responses for consensus"));
        }
        
        // For now, use weighted voting based on model confidence and weight
        let mut best_response = &responses[0];
        let mut best_score = 0.0;
        
        for (i, response) in responses.iter().enumerate() {
            if let Some(model_usage) = models_used.get(i) {
                let score = response.confidence as f64 * model_usage.contribution_weight;
                if score > best_score {
                    best_score = score;
                    best_response = response;
                }
            }
        }
        
        // Calculate ensemble confidence
        let total_weight: f64 = models_used.iter().map(|m| m.contribution_weight).sum();
        let weighted_confidence: f64 = responses.iter()
            .zip(models_used.iter())
            .map(|(resp, model)| resp.confidence as f64 * model.contribution_weight)
            .sum();
        
        let final_confidence = if total_weight > 0.0 {
            weighted_confidence / total_weight
        } else {
            best_response.confidence.into()
        };
        
        Ok((best_response.clone(), final_confidence))
    }

    async fn execute_sequential_chaining(&self, request: &OrchestrationRequest, decision: &OrchestrationDecision) -> Result<OrchestrationResponse> {
        // For now, fall back to single best
        self.execute_single_best(request, decision).await
    }

    async fn execute_specialized_division(&self, request: &OrchestrationRequest, decision: &OrchestrationDecision) -> Result<OrchestrationResponse> {
        // For now, fall back to single best
        self.execute_single_best(request, decision).await
    }

    async fn execute_adaptive_orchestration(&self, request: &OrchestrationRequest, decision: &OrchestrationDecision) -> Result<OrchestrationResponse> {
        // Choose strategy based on current conditions
        let adaptive_strategy = if request.enable_consensus && decision.primary_models.len() > 1 {
            OrchestrationStrategy::ParallelConsensus
        } else {
            OrchestrationStrategy::SingleBest
        };
        
        let mut adaptive_decision = decision.clone();
        adaptive_decision.strategy = adaptive_strategy.clone();
        
        match adaptive_strategy {
            OrchestrationStrategy::ParallelConsensus => {
                self.execute_parallel_consensus(request, &adaptive_decision).await
            },
            _ => {
                self.execute_single_best(request, &adaptive_decision).await
            }
        }
    }

    async fn update_orchestration_metrics(&self, response: &OrchestrationResponse) -> Result<()> {
        let mut metrics = self.metrics.write().await;
        
        metrics.total_requests += 1;
        if response.final_confidence > 0.5 {
            metrics.successful_requests += 1;
        }
        
        // Update averages
        let total = metrics.total_requests as f64;
        metrics.average_response_time = (metrics.average_response_time * (total - 1.0) + response.total_time_ms as f64) / total;
        metrics.average_cost = (metrics.average_cost * (total - 1.0) + response.total_cost) / total;
        metrics.average_confidence = (metrics.average_confidence * (total - 1.0) + response.final_confidence) / total;
        
        // Update strategy effectiveness
        let current_effectiveness = metrics.strategy_effectiveness.get(&response.strategy_used).copied().unwrap_or(0.0);
        let strategy_count = 1.0; // Simplified for now
        let new_effectiveness = (current_effectiveness * (strategy_count - 1.0) + response.final_confidence) / strategy_count;
        metrics.strategy_effectiveness.insert(response.strategy_used.clone(), new_effectiveness);
        
        Ok(())
    }

    async fn store_orchestration_history(&self, response: OrchestrationResponse) -> Result<()> {
        let mut history = self.orchestration_history.write().await;
        history.push_back(response);
        
        // Keep history size manageable
        while history.len() > 1000 {
            history.pop_front();
        }
        
        Ok(())
    }

    /// Get orchestration metrics
    pub async fn get_metrics(&self) -> Result<OrchestrationMetrics> {
        Ok(self.metrics.read().await.clone())
    }

    /// Get model performance data
    pub async fn get_model_performance(&self) -> Result<HashMap<String, ModelPerformance>> {
        Ok(self.model_performance.read().await.clone())
    }
}

impl Clone for MultiModelOrchestrator {
    fn clone(&self) -> Self {
        Self {
            config: self.config.clone(),
            llm_integration: Arc::clone(&self.llm_integration),
            learning_engine: self.learning_engine.clone(),
            error_recovery: self.error_recovery.clone(),
            model_performance: Arc::clone(&self.model_performance),
            orchestration_history: Arc::clone(&self.orchestration_history),
            metrics: Arc::clone(&self.metrics),
            request_semaphore: Arc::clone(&self.request_semaphore),
            strategy_cache: Arc::clone(&self.strategy_cache),
        }
    }
}

/// Create multi-model orchestrator
pub async fn create_multi_model_orchestrator(
    llm_integration: Arc<LLMIntegrationManager>,
) -> Result<MultiModelOrchestrator> {
    let orchestrator = MultiModelOrchestrator::new(
        OrchestrationConfig::default(),
        llm_integration,
    );
    
    orchestrator.initialize().await?;
    Ok(orchestrator)
}

/// Create orchestrator with custom configuration
pub async fn create_custom_orchestrator(
    config: OrchestrationConfig,
    llm_integration: Arc<LLMIntegrationManager>,
) -> Result<MultiModelOrchestrator> {
    let orchestrator = MultiModelOrchestrator::new(config, llm_integration);
    orchestrator.initialize().await?;
    Ok(orchestrator)
}

/// Create fully integrated orchestrator
pub async fn create_integrated_orchestrator(
    llm_integration: Arc<LLMIntegrationManager>,
    learning_engine: Arc<AdvancedLearningEngine>,
    error_recovery: Arc<ErrorRecoveryManager>,
    config: OrchestrationConfig,
) -> Result<MultiModelOrchestrator> {
    let orchestrator = MultiModelOrchestrator::new(config, llm_integration)
        .with_learning_engine(learning_engine)
        .with_error_recovery(error_recovery);
    
    orchestrator.initialize().await?;
    Ok(orchestrator)
}